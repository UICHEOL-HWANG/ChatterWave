import torch
from transformers import (
    Trainer,
    TrainingArguments,
    pipeline
)
from sklearn.metrics import f1_score, precision_score, recall_score
import mlflow
from tracking.save_registry import SaveTracking
import os 

tracking = SaveTracking()

os.environ["MLFLOW_S3_ENDPOINT_URL"] = tracking.mlflow_s3_endpoint
os.environ["MLFLOW_TRACKING_URI"] = tracking.mlflow_tracking_uri
os.environ["AWS_ACCESS_KEY_ID"] =  tracking.minio_access_key
os.environ["AWS_SECRET_ACCESS_KEY"] = tracking.minio_secret_key

# GPU í™•ì¸ í•¨ìˆ˜
def check_device():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.cuda.is_available():
        print(f"âœ… GPU Detected: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ No GPU Detected. Using CPU instead.")
    return device


# í‰ê°€ ì§€í‘œ í•¨ìˆ˜
def compute_metrics(eval_pred):
    """
    ê²€ì¦ ì‹œ F1 Score, Precision, Recallì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), dim=-1).numpy()
    labels = labels
    
    f1 = f1_score(labels, preds, average='macro')
    precision = precision_score(labels, preds, average='macro')
    recall = recall_score(labels, preds, average='macro')
    
    return {
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


# TrainingManager í´ë˜ìŠ¤
class TrainingManager:
    def __init__(self, model, tokenizer, learning_rate, experiment, epochs=5):
        self.learning_rate = learning_rate
        self.model = model
        self.tokenizer = tokenizer
        self.device = check_device()
        self.model.to(self.device)
        self.epochs = epochs
        self.experiment = experiment
        
        # ì‹¤í—˜ ì„¸íŒ…
        mlflow.set_tracking_uri(tracking.mlflow_tracking_uri)
        mlflow.set_experiment(self.experiment)
        print(f"ì‹¤í—˜ ì„¤ì • ì™„ë£Œ ì‹¤í—˜ë²„ì „ëª… {self.experiment}")
    
    def train(self, train_dataset, 
              valid_dataset, 
              output_dir, 
              train_batch_size, 
              valid_batch_size,
              weight_decay,
              grad_norm,
              logging_dir
              ):
        """
        ëª¨ë¸ í•™ìŠµ, ê²€ì¦ ë° ìµœì  ëª¨ë¸ ì €ì¥
        """
        training_args = TrainingArguments(
            output_dir=output_dir,             # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            num_train_epochs=self.epochs,       # í•™ìŠµ Epoch ìˆ˜
            per_device_train_batch_size=train_batch_size,     # GPUë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°
            per_device_eval_batch_size=valid_batch_size,      # GPUë‹¹ ê²€ì¦ ë°°ì¹˜ í¬ê¸°
            warmup_steps=100,                   # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì„ ìœ„í•œ ì›œì—… ìŠ¤í…
            weight_decay=weight_decay,                  # ê°€ì¤‘ì¹˜ ê°ì†Œ
            logging_dir=logging_dir,               # ë¡œê·¸ ì €ì¥ ê²½ë¡œ
            logging_steps=10,                   # ë¡œê·¸ ì¶œë ¥ ê°„ê²©
            evaluation_strategy='steps',        # ë§¤ Epochë§ˆë‹¤ Validation ì‹¤í–‰
            eval_steps=500,
            save_strategy='steps',              # ë§¤ Epochë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            load_best_model_at_end=True,        # ìµœì ì˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
            metric_for_best_model='f1',         # ìµœì  ëª¨ë¸ ê¸°ì¤€
            fp16=True if torch.cuda.is_available() else False,  # Mixed Precision í™œì„±í™”
            report_to='mlflow',                   # mlflowë¡œ ì„¸íŒ…
            logging_first_step=True,             # ì²« ìŠ¤í…ë¶€í„° ë¡œê·¸ ì¶œë ¥
            max_grad_norm=grad_norm 
        )
        
        # Trainer ê°ì²´ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=valid_dataset,
            tokenizer=self.tokenizer,
            compute_metrics=compute_metrics
        )
        
        with mlflow.start_run():
            mlflow.log_param("learning_rate", self.learning_rate)
            mlflow.log_param("epochs", self.epochs)
            mlflow.log_param("train_batch_size", train_batch_size)
            mlflow.log_param("valid_batch_size", valid_batch_size)
            mlflow.log_param("max_grad_norm", grad_norm)

            
            
            # í•™ìŠµ ì‹¤í–‰
            print("ğŸš€ Starting Training...")
            training = trainer.train()

            mlflow.log_metric("train_loss", training.metrics["train_loss"])
            mlflow.log_metric("train_runtime", training.metrics["train_runtime"])
            mlflow.log_metric("train_samples_per_second", training.metrics["train_samples_per_second"])
        
            # ê²€ì¦ ì‹¤í–‰
            print("ğŸ“Š Running Validation...")
            eval_results = trainer.evaluate()
            print("âœ… Validation Results:")
            for key, value in eval_results.items():
                print(f"{key}: {value:.4f}")
                # ë¡œê¹… ê²°ê³¼ ìˆ˜ì§‘ 
                mlflow.log_metric(key, value)
        
            # ìµœì  ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
            print("ğŸ’¾ Saving Best Model and Tokenizer...")
            trainer.save_model(output_dir)
            self.tokenizer.save_pretrained(output_dir)
            print(f"âœ… Model and Tokenizer Saved at {output_dir}")
            
            task = "text-classification"
            
            sentence_pipeline = pipeline(
                task=task,
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1 
            )
            
            mlflow.transformers.log_model(
                transformers_model=sentence_pipeline,
                artifact_path="outputs",
                task=task,
                registered_model_name=self.experiment
            )
            
            print(f"ì•„í‹°íŒ©íŠ¸ ì €ì¥ì™„ë£Œ: {self.experiment}/outputs")
            
        return eval_results
